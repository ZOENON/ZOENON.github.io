<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jiangcheng Song (宋蒋成)</title>

  <meta name="author" content="Jiangcheng Son">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Jiangcheng Song (宋蒋成)
                  </p>
                  <p>
                    I am an incoming Ph.D. student (starting Fall 2026) at the <a href="http://www.aiar.xjtu.edu.cn/">Institute of Artificial Intelligence and Robotics (IAIR)</a>,
                      <a href="http://www.xjtu.edu.cn/">Xi'an Jiaotong University</a>, advised by <a href="https://gr.xjtu.edu.cn/web/nnzheng"></a>Prof. Nanning Zheng</a>.
                  </p>
                  <p>
                    My research interests include <strong>Knowledge Distillation</strong>, <strong>Large Language
                      Models</strong>, <strong>Efficient Inference</strong>, and <strong>Dataset Condensation</strong>.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:enone@stu.xjtu.edu.cn">Email</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com.hk/citations?user=k3rKepYAAAAJ&hl=zh-CN">Google Scholar</a>
                    &nbsp;/&nbsp;
                    <a href="https://github.com/ZOENON">GitHub</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:37%;max-width:37%">
                  <a href="images/WechatIMG328.jpg"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="images/WechatIMG328.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Publications Section -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Publications</h2>
                  <p>
                    I am interested in efficient deep learning, knowledge distillation, and large language model
                    optimization. Representative papers are <span class="highlight">highlighted</span>.
                    <br>
                    <span style="color:#666; font-size:13px;">* indicates equal contribution</span>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <!-- Paper 1: DR-DPO -->
              <tr>
                <td style="padding:16px;width:20%;vertical-align:top">
                  <div class="one">
                    <img src='images/DRDPO.png' width=160
                      onerror="this.style.backgroundColor='#f0f0f0'; this.style.height='120px';">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <span class="papertitle">DR-DPO: Dual-Regularized DPO for Efficient Dataset Condensation</span>
                  <br>
                  <a href="#">Haiduo Huang</a>*,
                  <strong>Jiangcheng Song</strong>*,
                  <a href="#">Yadong Zhang</a>*,
                  <a href="#">Guansu Wang</a>,
                  <a href="#">Pengju Ren</a>
                  <br>
                  <span style="color:#000000; font-size:13px;">* Three authors contributed equally</span>
                  <br>
                  <em>Under Review</em>
                  <br>
                  <p></p>
                  <p>
                    A dual-regularized framework that recasts dataset condensation as preference optimization. An
                    inter-class DPO loss maximizes class separability while an intra-class Jensen-Shannon term preserves
                    within-class statistics.
                  </p>
                </td>
              </tr>

              <!-- Paper 2: SelecTKD -->
              <tr bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:top">
                  <div class="one">
                    <img src='images/selectkd.png' width=160
                      onerror="this.style.backgroundColor='#f0f0f0'; this.style.height='120px';">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2510.24021">
                    <span class="papertitle">SelecTKD: Selective Token-Weighted Knowledge Distillation for LLMs</span>
                  </a>
                  <br>
                  <a href="#">Haiduo Huang</a>*,
                  <strong>Jiangcheng Song</strong>*,
                  <a href="#">Yadong Zhang</a>*,
                  <a href="#">Pengju Ren</a>
                  <br>
                  <span style="color:#000000; font-size:13px;">* Three authors contributed equally</span>
                  <br>
                  <em>Under Review</em>
                  <br>
                  <a href="https://arxiv.org/abs/2510.24021">arXiv</a>
                  <p></p>
                  <p>
                    A selective token-weighted knowledge distillation method for large language models that adaptively
                    weights tokens during distillation for improved efficiency and performance.
                  </p>
                </td>
              </tr>

              <!-- Paper 3: FastEagle -->
              <tr>
                <td style="padding:16px;width:20%;vertical-align:top">
                  <div class="one">
                    <img src='images/fasteagle.png' width=160
                      onerror="this.style.backgroundColor='#f0f0f0'; this.style.height='120px';">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2509.20416">
                    <span class="papertitle">FastEagle: Cascaded Drafting for Accelerating Speculative Decoding</span>
                  </a>
                  <br>
                  <a href="#">Haiduo Huang</a>*,
                  <strong>Jiangcheng Song</strong>*,
                  <a href="#">Wenzhe Zhao</a>,
                  <a href="#">Pengju Ren</a>
                  <br>
                  <span style="color:#000000; font-size:13px;">* Three authors contributed equally</span>
                  <br>
                  <em>ICASSP</em>, 2026
                  <br>
                  <a href="https://arxiv.org/abs/2509.20416">arXiv</a>
                  <p></p>
                  <p>
                    A cascaded drafting approach for accelerating speculative decoding in large language models,
                    enabling faster inference without sacrificing quality.
                  </p>
                </td>
              </tr>

              <!-- Paper 4: PPDD -->
              <tr>
                <td style="padding:16px;width:20%;vertical-align:top">
                  <div class="one">
                    <img src='images/ppdd.png' width=160
                      onerror="this.style.backgroundColor='#f0f0f0'; this.style.height='120px';">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <span class="papertitle">PPDD: A Unified Push-Pull Adversarial Objective in Feature and Logit Spaces
                    for Dataset Distillation</span>
                  <br>
                  <a href="#">Haiduo Huang</a>*,
                  <a href="#">Yadong Zhang</a>*,
                  <strong>Jiangcheng Song</strong>*,
                  <a href="#">Wenzhe Zhao</a>,
                  <a href="#">Pengju Ren</a>
                  <br>
                  <span style="color:#000000; font-size:13px;">* Three authors contributed equally</span>
                  <br>
                  <em>ICASSP</em>, 2026
                  <br>
                  <p></p>
                  <p>
                    A unified push-pull adversarial objective that operates in both feature and logit spaces for
                    effective dataset distillation.
                  </p>
                </td>
              </tr>

              <!-- Paper 5: DeepKD -->
              <tr bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:top">
                  <div class="one">
                    <img src='images/deepkd.png' width=160
                      onerror="this.style.backgroundColor='#f0f0f0'; this.style.height='120px';">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2505.15133">
                    <span class="papertitle">DeepKD: A Deeply Decoupled and Denoised Knowledge Distillation
                      Trainer</span>
                  </a>
                  <br>
                  <a href="#">Haiduo Huang</a>*,
                  <strong>Jiangcheng Song</strong>*,
                  <a href="#">Yadong Zhang</a>*,
                  <a href="#">Pengju Ren</a>
                  <br>
                  <span style="color:#000000; font-size:13px;">* Three authors contributed equally</span>
                  <br>
                  <em>NeurIPS</em>, 2025
                  <br>
                  <a href="https://arxiv.org/abs/2505.15133">arXiv</a>
                  /
                  <a href="https://github.com/haiduo/DeepKD">code</a>
                  <p></p>
                  <p>
                    A knowledge distillation framework that deeply decouples and denoises the distillation process for
                    improved training stability and performance.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>

          <!-- Collaboration Publications Section -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Collaboration Publications</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <!-- Collab Paper 1: Multi-Teacher CoT -->
              <tr>
                <td style="padding:16px;width:20%;vertical-align:top">
                  <div class="one">
                    <img src='images/multi_teacher_cot.png' width=160
                      onerror="this.style.backgroundColor='#f0f0f0'; this.style.height='120px';">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2601.13992">
                    <span class="papertitle">"The Whole Is Greater Than the Sum of Its Parts": A Compatibility-Aware
                      Multi-Teacher CoT Distillation Framework</span>
                  </a>
                  <br>
                  <a href="#">Jiazhen Cui</a>,
                  <a href="#">Jiahao Guo</a>,
                  <a href="#">Jie Zhou</a>,
                  <a href="#">Rui Yang</a>,
                  <a href="#">Jian Lu</a>,
                  <a href="#">Jun Xu</a>,
                  <strong>Jiangcheng Song</strong>,
                  <a href="#">Bing Zhao</a>,
                  <a href="#">Pengju Ren</a>
                  <br>
                  <em>Under Review</em>
                  <br>
                  <a href="https://arxiv.org/abs/2601.13992">arXiv</a>
                  <p></p>
                  <p>
                    A compatibility-aware multi-teacher chain-of-thought distillation framework that leverages the
                    complementary strengths of multiple teacher models.
                  </p>
                </td>
              </tr>

              <!-- Collab Paper 2: MIND -->
              <tr>
                <td style="padding:16px;width:20%;vertical-align:top">
                  <div class="one">
                    <img src='images/mind.png' width=160
                      onerror="this.style.backgroundColor='#f0f0f0'; this.style.height='120px';">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2601.03717">
                    <span class="papertitle">MIND: From Passive Mimicry to Active Reasoning through Capability-Aware
                      Multi-Perspective CoT Distillation</span>
                  </a>
                  <br>
                  <a href="#">Jiazhen Cui</a>,
                  <a href="#">Jiahao Guo</a>,
                  <a href="#">Jie Zhou</a>,
                  <a href="#">Rui Yang</a>,
                  <a href="#">Jian Lu</a>,
                  <a href="#">Jun Xu</a>,
                  <strong>Jiangcheng Song</strong>,
                  <a href="#">Bing Zhao</a>,
                  <a href="#">Pengju Ren</a>
                  <br>
                  <em>Under Review</em>
                  <br>
                  <a href="https://arxiv.org/abs/2601.03717">arXiv</a>
                  <p></p>
                  <p>
                    A capability-aware multi-perspective chain-of-thought distillation method that transforms passive
                    mimicry into active reasoning.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>

          <!-- Education Section -->
          <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;">
            <tbody>
              <tr>
                <td>
                  <h2>Education</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                  <img src="images/rjs_logo.png" width="80" onerror="this.style.display='none'">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <strong>Xi'an Jiaotong University</strong>
                  <br>
                  State Key Laboratory of Human-Machine Hybrid Augmented Intelligence
                  <br>
                  Institute of Artificial Intelligence and Robotics
                  <br>
                  <em>Research Intern</em>
                  <em><br>advisor: <a href="https://gr.xjtu.edu.cn/web/pengjuren/home">Professor Pengju Ren</a></em>
                  <br>
                  <em>9/2024 - 2/2026</em>
                </td>
              </tr>
              <tr>
                <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                  <img src="images/xjtuai.png" width="80" onerror="this.style.display='none'">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <strong>Xi'an Jiaotong University</strong>
                  <br>
                  College of artificial intelligence
                  <br>
                  State Key Laboratory of Human-Machine Hybrid Augmented Intelligence
                  <br>
                  Institute of Artificial Intelligence and Robotics
                  <br>
                  <em>PhD Student</em>
                  <br>
                  <em>advisor: <a href="https://gr.xjtu.edu.cn/web/nnzheng">Professor Nanning Zheng</a></em>
                  <br>
                  <em>9/2026 - </em>
                </td>
              </tr>

            </tbody>
          </table>

          
        </td>
      </tr>
  </table>
</body>

</html>